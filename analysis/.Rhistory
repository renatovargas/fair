# cada una de las palabras que más se repiten
# poniendo un umbral mínimo de 75%.
# Esto significa que estas palabras aparecen juntas
# al menos 25% del tiempo.
asoc <- findAssocs(dtm, c(corwords), corlimit=0.60)
# Escribimos las tablas de correlaciones de cada
# una de las palabras relevantes a pestanas de
# un Excel (corrPalabrasRelev.xlsx).
wb <- createWorkbook()
addWorksheet(wb, "dimensiones")
writeData(wb, "dimensiones", as.data.frame(dim(dtm), row.names=c("filas", "columnas")), rowNames=TRUE)
for (k in 1:length(asoc)) {
a <- as.data.frame(asoc[k])
addWorksheet(wb, names(a))
writeData(wb, names(a), a, rowNames = TRUE)
}
saveWorkbook(wb, "8_Obstaculos_corr.xlsx", overwrite=TRUE)
dtms <- removeSparseTerms(dtm, 0.93)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 3)
pdf(file="8_Obstaculos_cluster.pdf", bg="white")
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
dev.off()
# Hacemos conglomerados por el metodo de Ward
# Graficamos un dendrograma para identificar
# grupos estadísticamente independientes
fit <- hclust(d=d, method="ward.D")
pdf(file="8_Obstaculos_dendrograma.pdf", bg="white")
plot(fit, hang=-1, main=paste("Palabras ", sep=""), ylab="Niveles de Grupos", xlab="Grupos (Método de Ward)")
groups <- cutree(fit, k=7)
rect.hclust(fit, k=7, border="red")
dev.off()
dtms <- removeSparseTerms(dtm, 0.92)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 3)
pdf(file="8_Obstaculos_cluster.pdf", bg="white")
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
dev.off()
# Hacemos conglomerados por el metodo de Ward
# Graficamos un dendrograma para identificar
# grupos estadísticamente independientes
fit <- hclust(d=d, method="ward.D")
pdf(file="8_Obstaculos_dendrograma.pdf", bg="white")
plot(fit, hang=-1, main=paste("Palabras ", sep=""), ylab="Niveles de Grupos", xlab="Grupos (Método de Ward)")
groups <- cutree(fit, k=7)
rect.hclust(fit, k=7, border="red")
dev.off()
# 8. Obstaculos al participar en proyectos como este
# --------------------------------------------------
# Y nos quedamos solamente con las respuestas
txt <- fairdb$Si.su.respuesta.a.la.pregunta.anterior.es..Sí...cuáles.son.los.principales.obstáculos.que.enfrentó.
# Limpiamos el texto de puntuacion y palabras comunes como preposiciones y articulos
txt <- gsub("[[:punct:]]", "", txt)
txt <- gsub("[[:digit:]]", "", txt)
txt <- gsub("http\\w+", "", txt)
# Creamos un corpus (cuerpo de texto) para analizar
corpus <- Corpus(VectorSource(txt))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c(stopwords("spanish")))
# Removemos palabras que hacen ruido
corpus <- tm_map(corpus, removeWords, "proyecto")
corpus <- tm_map(corpus, removeWords, "proyectos")
corpus <- tm_map(corpus, removeWords, "ser")
corpus <- tm_map(corpus, removeWords, "solo")
corpus <- tm_map(corpus, removeWords, "temas")
corpus <- tm_map(corpus, removeWords, "falta")
# Corregimos errores de digitación y consolidamos palabras
# similares que se refieren a lo mismo en un solo término.
# Se restringe la búsqueda y reemplazo a lo que se encuentre
# dentro de \\b para no reemplazar palabras derivadas de
# manera equivocada o inesperada.
# for (j in seq(corpus)){
# 	 corpus[[j]] <- gsub("\\pequeños productores\\b", "pequeños_productores", corpus[[j]])
#  }
# corpus <- tm_map(corpus, removeWords, "jóvenes")
# Eliminamos el espacio en blanco
corpus <- tm_map(corpus, stripWhitespace)
# corpus <- tm_map(corpus, PlainTextDocument)
# Creamos una matriz de terminos
dtm <- DocumentTermMatrix(corpus)
tdm <- TermDocumentMatrix(corpus)
#writeLines(as.character(corpus[[31]]))
m <- as.matrix(tdm)
wf <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word = names(wf), freq=wf)
# Graficamos una nube de palabras para inspección visual
set.seed(124)
pdf(file= "8_Obstaculos_nube.pdf", bg="white")
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
# Creamos una tabla de frecuencias
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
# Graficamos palabras que aparecen al menos 6 veces
# en formato histograma
p <- ggplot(subset(wf, freq>6), aes(word,freq)) + geom_bar(stat="identity") + theme_bw() + theme(axis.text.x=element_text(angle = 45, hjust = 1))  + labs(x = "Palabras") + ylab("Frecuencia")
pdf(file="8_Obstaculos_histograma.pdf", bg="white")
print(p)
dev.off()
# Compactamos la matriz de análisis
# y graficamos los conglomerados
dtms <- removeSparseTerms(dtm, 0.92)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 3)
pdf(file="8_Obstaculos_cluster.pdf", bg="white")
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
dev.off()
# Hacemos conglomerados por el metodo de Ward
# Graficamos un dendrograma para identificar
# grupos estadísticamente independientes
fit <- hclust(d=d, method="ward.D")
pdf(file="8_Obstaculos_dendrograma.pdf", bg="white")
plot(fit, hang=-1, main=paste("Palabras ", sep=""), ylab="Niveles de Grupos", xlab="Grupos (Método de Ward)")
groups <- cutree(fit, k=7)
rect.hclust(fit, k=7, border="red")
dev.off()
# Extraemos las palabras relevantes que aparecen
# al menos 11 veces
corwords <- as.data.frame(subset(wf, freq>10))
corwords <- as.vector(corwords$word)
# Encontramos palabras con asociación más fuerte para
# cada una de las palabras que más se repiten
# poniendo un umbral mínimo de 75%.
# Esto significa que estas palabras aparecen juntas
# al menos 25% del tiempo.
asoc <- findAssocs(dtm, c(corwords), corlimit=0.60)
# Escribimos las tablas de correlaciones de cada
# una de las palabras relevantes a pestanas de
# un Excel (corrPalabrasRelev.xlsx).
wb <- createWorkbook()
addWorksheet(wb, "dimensiones")
writeData(wb, "dimensiones", as.data.frame(dim(dtm), row.names=c("filas", "columnas")), rowNames=TRUE)
for (k in 1:length(asoc)) {
a <- as.data.frame(asoc[k])
addWorksheet(wb, names(a))
writeData(wb, names(a), a, rowNames = TRUE)
}
saveWorkbook(wb, "8_Obstaculos_corr.xlsx", overwrite=TRUE)
# Y nos quedamos solamente con las respuestas
txt <- fairdb$X.Cómo.se.podrían.mejorar.estos.proyectos..Oportunidades.de.mejora
# Limpiamos el texto de puntuacion y palabras comunes como preposiciones y articulos
txt <- gsub("[[:punct:]]", "", txt)
txt <- gsub("[[:digit:]]", "", txt)
txt <- gsub("http\\w+", "", txt)
# Creamos un corpus (cuerpo de texto) para analizar
corpus <- Corpus(VectorSource(txt))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c(stopwords("spanish")))
# Removemos palabras
corpus <- tm_map(corpus, removeWords, "proyecto")
corpus <- tm_map(corpus, removeWords, "proyectos")
corpus <- tm_map(corpus, removeWords, "tener")
corpus <- tm_map(corpus, removeWords, "nivel")
corpus <- tm_map(corpus, removeWords, "hacer")
corpus <- tm_map(corpus, removeWords, "ser")
# Corregimos errores de digitación y consolidamos palabras
# similares que se refieren a lo mismo en un solo término.
# Se restringe la búsqueda y reemplazo a lo que se encuentre
# dentro de \\b para no reemplazar palabras derivadas de
# manera equivocada o inesperada.
# for (j in seq(corpus)){
# 	 corpus[[j]] <- gsub("\\pequeños productores\\b", "pequeños_productores", corpus[[j]])
#  }
# corpus <- tm_map(corpus, removeWords, "jóvenes")
# Eliminamos el espacio en blanco
corpus <- tm_map(corpus, stripWhitespace)
# corpus <- tm_map(corpus, PlainTextDocument)
# Creamos una matriz de terminos
dtm <- DocumentTermMatrix(corpus)
tdm <- TermDocumentMatrix(corpus)
#writeLines(as.character(corpus[[31]]))
m <- as.matrix(tdm)
wf <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word = names(wf), freq=wf)
# Graficamos una nube de palabras para inspección visual
set.seed(124)
pdf(file= "9_Oportunidades_nube.pdf", bg="white")
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
# Creamos una tabla de frecuencias
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
# Graficamos palabras que aparecen al menos 6 veces
# en formato histograma
p <- ggplot(subset(wf, freq>6), aes(word,freq)) + geom_bar(stat="identity") + theme_bw() + theme(axis.text.x=element_text(angle = 45, hjust = 1))  + labs(x = "Palabras") + ylab("Frecuencia")
pdf(file="9_Oportunidades_histograma.pdf", bg="white")
print(p)
dev.off()
# Compactamos la matriz de análisis
# y graficamos los conglomerados
dtms <- removeSparseTerms(dtm, 0.93)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 3)
pdf(file="9_Oportunidades_cluster.pdf", bg="white")
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
dev.off()
# Hacemos conglomerados por el metodo de Ward
# Graficamos un dendrograma para identificar
# grupos estadísticamente independientes
fit <- hclust(d=d, method="ward.D")
pdf(file="9_Oportunidades_dendrograma.pdf", bg="white")
plot(fit, hang=-1, main=paste("Palabras ", sep=""), ylab="Niveles de Grupos", xlab="Grupos (Método de Ward)")
groups <- cutree(fit, k=7)
rect.hclust(fit, k=7, border="red")
dev.off()
# Extraemos las palabras relevantes que aparecen
# al menos 11 veces
corwords <- as.data.frame(subset(wf, freq>10))
corwords <- as.vector(corwords$word)
# Encontramos palabras con asociación más fuerte para
# cada una de las palabras que más se repiten
# poniendo un umbral mínimo de 75%.
# Esto significa que estas palabras aparecen juntas
# al menos 25% del tiempo.
asoc <- findAssocs(dtm, c(corwords), corlimit=0.60)
# Escribimos las tablas de correlaciones de cada
# una de las palabras relevantes a pestanas de
# un Excel (corrPalabrasRelev.xlsx).
wb <- createWorkbook()
addWorksheet(wb, "dimensiones")
writeData(wb, "dimensiones", as.data.frame(dim(dtm), row.names=c("filas", "columnas")), rowNames=TRUE)
for (k in 1:length(asoc)) {
a <- as.data.frame(asoc[k])
addWorksheet(wb, names(a))
writeData(wb, names(a), a, rowNames = TRUE)
}
saveWorkbook(wb, "9_Oportunidades_corr.xlsx", overwrite=TRUE)
# Y nos quedamos solamente con las respuestas
txt <- fairdb$X.Cómo.se.podrían.mejorar.estos.proyectos..Oportunidades.de.mejora
# Limpiamos el texto de puntuacion y palabras comunes como preposiciones y articulos
txt <- gsub("[[:punct:]]", "", txt)
txt <- gsub("[[:digit:]]", "", txt)
txt <- gsub("http\\w+", "", txt)
# Creamos un corpus (cuerpo de texto) para analizar
corpus <- Corpus(VectorSource(txt))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c(stopwords("spanish")))
# Removemos palabras
corpus <- tm_map(corpus, removeWords, "proyecto")
corpus <- tm_map(corpus, removeWords, "proyectos")
corpus <- tm_map(corpus, removeWords, "tener")
corpus <- tm_map(corpus, removeWords, "nivel")
corpus <- tm_map(corpus, removeWords, "hacer")
corpus <- tm_map(corpus, removeWords, "ser")
# Corregimos errores de digitación y consolidamos palabras
# similares que se refieren a lo mismo en un solo término.
# Se restringe la búsqueda y reemplazo a lo que se encuentre
# dentro de \\b para no reemplazar palabras derivadas de
# manera equivocada o inesperada.
# for (j in seq(corpus)){
# 	 corpus[[j]] <- gsub("\\pequeños productores\\b", "pequeños_productores", corpus[[j]])
#  }
# corpus <- tm_map(corpus, removeWords, "jóvenes")
# Eliminamos el espacio en blanco
corpus <- tm_map(corpus, stripWhitespace)
# corpus <- tm_map(corpus, PlainTextDocument)
# Creamos una matriz de terminos
dtm <- DocumentTermMatrix(corpus)
tdm <- TermDocumentMatrix(corpus)
#writeLines(as.character(corpus[[31]]))
m <- as.matrix(tdm)
wf <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word = names(wf), freq=wf)
# Graficamos una nube de palabras para inspección visual
set.seed(124)
pdf(file= "9_Oportunidades_nube.pdf", bg="white")
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
# Creamos una tabla de frecuencias
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
# Graficamos palabras que aparecen al menos 6 veces
# en formato histograma
p <- ggplot(subset(wf, freq>6), aes(word,freq)) + geom_bar(stat="identity") + theme_bw() + theme(axis.text.x=element_text(angle = 45, hjust = 1))  + labs(x = "Palabras") + ylab("Frecuencia")
pdf(file="9_Oportunidades_histograma.pdf", bg="white")
print(p)
dev.off()
# Compactamos la matriz de análisis
# y graficamos los conglomerados
dtms <- removeSparseTerms(dtm, 0.93)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 3)
pdf(file="9_Oportunidades_cluster.pdf", bg="white")
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
dev.off()
# Hacemos conglomerados por el metodo de Ward
# Graficamos un dendrograma para identificar
# grupos estadísticamente independientes
fit <- hclust(d=d, method="ward.D")
pdf(file="9_Oportunidades_dendrograma.pdf", bg="white")
plot(fit, hang=-1, main=paste("Palabras ", sep=""), ylab="Niveles de Grupos", xlab="Grupos (Método de Ward)")
groups <- cutree(fit, k=7)
rect.hclust(fit, k=7, border="red")
dev.off()
# Extraemos las palabras relevantes que aparecen
# al menos 11 veces
corwords <- as.data.frame(subset(wf, freq>10))
corwords <- as.vector(corwords$word)
# Encontramos palabras con asociación más fuerte para
# cada una de las palabras que más se repiten
# poniendo un umbral mínimo de 75%.
# Esto significa que estas palabras aparecen juntas
# al menos 25% del tiempo.
asoc <- findAssocs(dtm, c(corwords), corlimit=0.60)
# Escribimos las tablas de correlaciones de cada
# una de las palabras relevantes a pestanas de
# un Excel (corrPalabrasRelev.xlsx).
wb <- createWorkbook()
addWorksheet(wb, "dimensiones")
writeData(wb, "dimensiones", as.data.frame(dim(dtm), row.names=c("filas", "columnas")), rowNames=TRUE)
for (k in 1:length(asoc)) {
a <- as.data.frame(asoc[k])
addWorksheet(wb, names(a))
writeData(wb, names(a), a, rowNames = TRUE)
}
saveWorkbook(wb, "9_Oportunidades_corr.xlsx", overwrite=TRUE)
# 13. Factores
# ------------
# Unimos los tres factores
fairdb$factores <- paste(fairdb$Factor.1, fairdb$Factor.2, fairdb$Factor.3, sep=" ")
# Y nos quedamos solamente con las respuestas
txt <- fairdb$factores
# Limpiamos el texto de puntuacion y palabras comunes como preposiciones y articulos
txt <- gsub("[[:punct:]]", "", txt)
txt <- gsub("[[:digit:]]", "", txt)
txt <- gsub("http\\w+", "", txt)
# Creamos un corpus (cuerpo de texto) para analizar
corpus <- Corpus(VectorSource(txt))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c(stopwords("spanish")))
# Removemos palabras
corpus <- tm_map(corpus, removeWords, "proyecto")
corpus <- tm_map(corpus, removeWords, "proyectos")
corpus <- tm_map(corpus, removeWords, "tener")
# Corregimos errores de digitación y consolidamos palabras
# similares que se refieren a lo mismo en un solo término.
# Se restringe la búsqueda y reemplazo a lo que se encuentre
# dentro de \\b para no reemplazar palabras derivadas de
# manera equivocada o inesperada.
# for (j in seq(corpus)){
# 	 corpus[[j]] <- gsub("\\pequeños productores\\b", "pequeños_productores", corpus[[j]])
#  }
# corpus <- tm_map(corpus, removeWords, "jóvenes")
# Eliminamos el espacio en blanco
corpus <- tm_map(corpus, stripWhitespace)
# corpus <- tm_map(corpus, PlainTextDocument)
# Creamos una matriz de terminos
dtm <- DocumentTermMatrix(corpus)
tdm <- TermDocumentMatrix(corpus)
#writeLines(as.character(corpus[[31]]))
m <- as.matrix(tdm)
wf <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word = names(wf), freq=wf)
# Graficamos una nube de palabras para inspección visual
set.seed(124)
pdf(file= "13_Factores_nube.pdf", bg="white")
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
# Creamos una tabla de frecuencias
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
# Graficamos palabras que aparecen al menos 6 veces
# en formato histograma
p <- ggplot(subset(wf, freq>7), aes(word,freq)) + geom_bar(stat="identity") + theme_bw() + theme(axis.text.x=element_text(angle = 45, hjust = 1))  + labs(x = "Palabras") + ylab("Frecuencia")
pdf(file="13_Factores_histograma.pdf", bg="white")
print(p)
dev.off()
# Compactamos la matriz de análisis
# y graficamos los conglomerados
dtms <- removeSparseTerms(dtm, 0.90)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 3)
pdf(file="13_Factores_cluster.pdf", bg="white")
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
dev.off()
# Hacemos conglomerados por el metodo de Ward
# Graficamos un dendrograma para identificar
# grupos estadísticamente independientes
fit <- hclust(d=d, method="ward.D")
pdf(file="13_Factores_dendrograma.pdf", bg="white")
plot(fit, hang=-1, main=paste("Palabras ", sep=""), ylab="Niveles de Grupos", xlab="Grupos (Método de Ward)")
groups <- cutree(fit, k=7)
rect.hclust(fit, k=7, border="red")
dev.off()
# Extraemos las palabras relevantes que aparecen
# al menos 11 veces
corwords <- as.data.frame(subset(wf, freq>10))
corwords <- as.vector(corwords$word)
# Encontramos palabras con asociación más fuerte para
# cada una de las palabras que más se repiten
# poniendo un umbral mínimo de 75%.
# Esto significa que estas palabras aparecen juntas
# al menos 25% del tiempo.
asoc <- findAssocs(dtm, c(corwords), corlimit=0.60)
# Escribimos las tablas de correlaciones de cada
# una de las palabras relevantes a pestanas de
# un Excel (corrPalabrasRelev.xlsx).
wb <- createWorkbook()
addWorksheet(wb, "dimensiones")
writeData(wb, "dimensiones", as.data.frame(dim(dtm), row.names=c("filas", "columnas")), rowNames=TRUE)
for (k in 1:length(asoc)) {
a <- as.data.frame(asoc[k])
addWorksheet(wb, names(a))
writeData(wb, names(a), a, rowNames = TRUE)
}
saveWorkbook(wb, "13_Factores_corr.xlsx", overwrite=TRUE)
# 13. Factores
# ------------
# Unimos los tres factores
fairdb$factores <- paste(fairdb$Factor.1, fairdb$Factor.2, fairdb$Factor.3, sep=" ")
# Y nos quedamos solamente con las respuestas
txt <- fairdb$factores
# Limpiamos el texto de puntuacion y palabras comunes como preposiciones y articulos
txt <- gsub("[[:punct:]]", "", txt)
txt <- gsub("[[:digit:]]", "", txt)
txt <- gsub("http\\w+", "", txt)
# Creamos un corpus (cuerpo de texto) para analizar
corpus <- Corpus(VectorSource(txt))
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, c(stopwords("spanish")))
# Removemos palabras
corpus <- tm_map(corpus, removeWords, "proyecto")
corpus <- tm_map(corpus, removeWords, "proyectos")
corpus <- tm_map(corpus, removeWords, "tener")
corpus <- tm_map(corpus, removeWords, "hacer")
# Corregimos errores de digitación y consolidamos palabras
# similares que se refieren a lo mismo en un solo término.
# Se restringe la búsqueda y reemplazo a lo que se encuentre
# dentro de \\b para no reemplazar palabras derivadas de
# manera equivocada o inesperada.
# for (j in seq(corpus)){
# 	 corpus[[j]] <- gsub("\\pequeños productores\\b", "pequeños_productores", corpus[[j]])
#  }
# corpus <- tm_map(corpus, removeWords, "jóvenes")
# Eliminamos el espacio en blanco
corpus <- tm_map(corpus, stripWhitespace)
# corpus <- tm_map(corpus, PlainTextDocument)
# Creamos una matriz de terminos
dtm <- DocumentTermMatrix(corpus)
tdm <- TermDocumentMatrix(corpus)
#writeLines(as.character(corpus[[31]]))
m <- as.matrix(tdm)
wf <- sort(rowSums(m), decreasing=TRUE)
dm <- data.frame(word = names(wf), freq=wf)
# Graficamos una nube de palabras para inspección visual
set.seed(124)
pdf(file= "13_Factores_nube.pdf", bg="white")
wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, "Dark2"))
dev.off()
# Creamos una tabla de frecuencias
freq <- colSums(as.matrix(dtm))
length(freq)
ord <- order(freq)
wf <- data.frame(word=names(freq), freq=freq)
head(wf)
# Graficamos palabras que aparecen al menos 6 veces
# en formato histograma
p <- ggplot(subset(wf, freq>7), aes(word,freq)) + geom_bar(stat="identity") + theme_bw() + theme(axis.text.x=element_text(angle = 45, hjust = 1))  + labs(x = "Palabras") + ylab("Frecuencia")
pdf(file="13_Factores_histograma.pdf", bg="white")
print(p)
dev.off()
# Compactamos la matriz de análisis
# y graficamos los conglomerados
dtms <- removeSparseTerms(dtm, 0.90)
d <- dist(t(dtms), method="euclidian")
kfit <- kmeans(d, 3)
pdf(file="13_Factores_cluster.pdf", bg="white")
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
dev.off()
# Hacemos conglomerados por el metodo de Ward
# Graficamos un dendrograma para identificar
# grupos estadísticamente independientes
fit <- hclust(d=d, method="ward.D")
pdf(file="13_Factores_dendrograma.pdf", bg="white")
plot(fit, hang=-1, main=paste("Palabras ", sep=""), ylab="Niveles de Grupos", xlab="Grupos (Método de Ward)")
groups <- cutree(fit, k=7)
rect.hclust(fit, k=7, border="red")
dev.off()
# Extraemos las palabras relevantes que aparecen
# al menos 11 veces
corwords <- as.data.frame(subset(wf, freq>10))
corwords <- as.vector(corwords$word)
# Encontramos palabras con asociación más fuerte para
# cada una de las palabras que más se repiten
# poniendo un umbral mínimo de 75%.
# Esto significa que estas palabras aparecen juntas
# al menos 25% del tiempo.
asoc <- findAssocs(dtm, c(corwords), corlimit=0.60)
# Escribimos las tablas de correlaciones de cada
# una de las palabras relevantes a pestanas de
# un Excel (corrPalabrasRelev.xlsx).
wb <- createWorkbook()
addWorksheet(wb, "dimensiones")
writeData(wb, "dimensiones", as.data.frame(dim(dtm), row.names=c("filas", "columnas")), rowNames=TRUE)
for (k in 1:length(asoc)) {
a <- as.data.frame(asoc[k])
addWorksheet(wb, names(a))
writeData(wb, names(a), a, rowNames = TRUE)
}
saveWorkbook(wb, "13_Factores_corr.xlsx", overwrite=TRUE)
